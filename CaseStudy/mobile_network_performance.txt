Mobile Network Performance

Components in mobile network
    - Baseband Processor
    - Cell Site (Transceiver base station, cell tower, access point)
    - Backhaul Network
        * dedicated WAN connection between a cell site, its controller, and the core network
    - Radio Network Controller
        * manages the neighboring cell sites and the mobile devices they service.
    - Core Network
        * serves as the gateway between the carrier’s private network and the public Internet
        * carriers employ in-line networking equipment to enforce quality-of-service policies or bandwidth metering

Two processors inside most mobile devices
    - Application processor 
        * is responsible for hosting the operating system and applications, and is analogous to your computer or laptop. 
    - Baseband processor 
        * responsible for all wireless network functions, and is analogous to a computer modem that uses radio waves instead of a phone line.
        * usually neglible latency source

Cell site:
    - provide network coverage within a region known as a cell.
    - simultaneously service hundreds to thousands of mobile devices

Transmission Control Protocol (TCP)
    - TCP is a session-oriented network transport built atop the conventions of IP networking. 
    - TCP affects the error-free duplex communications channel essential to other protocols such as HTTP or TLS.
    - TCP demonstrates a lot of the round-trip messaging we are trying to avoid. 
        * Some can be eliminated with adoption of protocol extensions like Fast Open. 
        * Others can be minimized by tuning system parameters, such as the initial congestion window. 

TCP three-way handshake (3WHS)
    - Initiating a TCP connection involves a 3-part message exchange convention known as the three-way handshake.  
    - negotiates the operating parameters between client and server that make robust 2-way communication possible. 
        * Initial SYN message represents the client’s connection request. 
        * Provided the server accepts the connection attempt it will reply with a SYN-ACK message. 
        * Finally, the client acknowledges the server with an ACK message. 
            - At this point a logical connection has been formed and the client may begin sending data. 
     - 3-way handshake introduces a delay equivalent to the current round-trip time.

TCP Fast Open (TFO)
    - TFO is an extension to TCP that eliminates the round-trip delay normally caused by the handshake process.
    - TFO allows the client to start sending data before the connection is logically established.
    - TCP Fast Open is a modification to the three-way handshake allowing a small data payload (e.g., HTTP request) to be placed within the SYN message. 
        * This payload is passed to the application server while the connection handshake is completed as it would otherwise. 
    - This effectively negates any round-trip delay from the three-way handshake. 
        * TFO can reduce page load times by as much as 40%. 
    - Although still only a draft specification, TFO is already supported by major browsers (Chrome 22+) and platforms (Linux 3.6+), with other vendors pledging to fully support it soon.

TFO's security concern
    - Earlier extension proposals like TFO ultimately failed due to security concerns. 
    - TFO addresses this issue with the notion of a secure token, or cookie, and expected to be included in the SYN message of a TFO-optimized request.
        * Secure token/cookie is  assigned to the client during the course of a conventional TCP connection handshake
    - SYNs with Spoofed IP Addresses
        * Standard TCP suffers from the SYN flood attack
            - because SYN packets with spoofed source IP addresses can easily fill up a listener's small queue, causing a service port to be blocked completely until timeouts.
    - TFO goes one step further to allow server-side TCP to send up data to the application layer before 3WHS is completed. 
        * This opens up serious new vulnerabilities. 
        * Applications serving ports that have TFO enabled may waste lots of CPU and memory resources processing the requests and producing the responses. 
    - MItigation of security concern:
        - server-supplied cookie to mitigatethese new vulnerabilities 

TFO deep dive:
    - The key component of TFO is the Fast Open Cookie (cookie), a message authentication code (MAC) tag generated by the server. 
    - The client requests a cookie in one regular TCP connection, then uses it for future TCP connections to exchange data during 3WHS:

Requesting a Fast Open Cookie:
   1. The client sends a SYN with a Fast Open Cookie Request option.
   2. The server generates a cookie and sends it through the Fast Open
      Cookie option of a SYN-ACK packet.
   3. The client caches the cookie for future TCP Fast Open connections
      (see below).

Performing TCP Fast Open:
   1. The client sends a SYN with Fast Open Cookie option and data.
   2. The server validates the cookie:
      a. If the cookie is valid, the server sends a SYN-ACK acknowledging both the SYN and the data. 
         The server then delivers the data to the application.
      b. Otherwise, the server drops the data and sends a SYN-ACK acknowledging only the SYN sequence number.
   3. If the server accepts the data in the SYN packet, it may send the
      response data before the handshake finishes. The max amount is
      governed by the TCP's congestion control 
   4. The client sends an ACK acknowledging the SYN and the server data.
      If the client's data is not acknowledged, the client retransmits the data in the ACK packet.
   5. The rest of the connection proceeds like a normal TCP connection.
      The client can repeat many Fast Open operations once it acquires a
      cookie (until the cookie is expired by the server). Thus TFO is
      useful for applications that have temporal locality on client and
      server connections.


Initial Congestion Window (initcwnd)
    - a configurable TCP setting with large potential to accelerate smaller network transactions.
    - Recent IETF specification promotes increasing the common initial congestion window setting of 3 segments (i.e., packets) to 10. 
        * demonstrates a 10% average performance boost. 
        * The purpose and potential impact of this setting can’t be appreciated without an introduction TCP’s congestion window (cwnd).
    - TCP guarantees reliability to client and server when operating over an otherwise unreliable network. 
        * This amounts to a promise that all data are received as they were sent, or at least appear to be. 
        * Packet loss is the largest obstacle to meeting the contract of reliablity; 
        * it requires detection, correction and prevention.
    - TCP employs a *positive acknowledgement convention* to detect missing packets in which every sent packet should be acknowledged by its intended receiver, 
        *the absence of which implies it was lost in transit. 
    - While awaiting acknowledgement, transmitted packets are preserved in a special buffer referred to as the congestion window. 
        * When this buffer becomes full, an event known as cwnd exhaustion, all transmission stops until receiver acknowledgements make room available to send more packets. 
        * These events play a significant role in TCP performance.
    - Apart from the limits of network bandwidth, TCP throughput is ultimately constrained by the frequency of cwnd exhaustion events
        * the likelihood of which relates to the size of the congestion window. 
        * Achieving peak TCP performance requires a congestion window complementing current network conditions
            - If cwnd is too large, it risks network congestion–an overcrowding condition marked by extensive packet loss; 
            - If cwnd is too small, precious bandwidth goes unused. 
    - Logically, the more known about network conditions the more likely an optimal congestion window will be chosen. 
        * Reality is that key network attributes, such as capacity and latency, are difficult to measure and constantly in flux. 
        * It complicates matters further that any Internet-based TCP connection will span across numerous networks. 
    *** a larger initial congestion window on the server accelerates downloads, whereas on the client it accelerates uploads.

TCP slow start
    - A fresh, or idle connection lacks evidence of the packet losses needed to establish an optimal congestion window size. 
        * TCP start with a congestion window with the least likelihood of causing congestion; 
            * this originally implied a setting of 1 segment (~1480 bytes)
        * In practice, initial congestion window is usually set to 3 segments (~4 KB).
      
Congestion avoidance
    - TCP will expand the congestion window just to the point it begins seeing packet loss, suggesting somewhere downstream there’s a network unable to handle the current transmission rate. 
        * Network congestion can’t be detected without signs of packet loss. 
    - Employing this congestion avoidance scheme, TCP eventually minimizes cwnd exhaustion events to the extent it consumes all the connection capacity it has been allotted.

Keepalive (HTTP pipelining)
    - Keepalive is an HTTP convention enabling use of the same TCP connection across sequential requests. 
    - At minimum a single round-trip–required for TCP’s three-way handshake is avoided
    - Further, keepalive has the additional, and often unheralded, performance advantage of preserving the current TCP congestion window between requests 
        * resulting in far fewer cwnd exhaustion events.    
    - HTTP pipelining
        * pipelining distributes the delay of a network round-trip amongst multiple HTTP transactions. 
            - For instance, 5 pipelined HTTP requests across a 100ms RTT connection will incur an average round-trip latency of 20ms. 
    - notable downsides to HTTP pipelining preventing its wider adoption
        * namely: a history of spotty HTTP proxy support and suceptibility to DoS attacks.

Transport Layer Security (TLS)
    - TLS is a session-oriented network protocol allowing sensitive information to be securely exchanged over a public network. 
    - While TLS is highly effective at securing communications its performance suffers when operating on high latency networks.
    - TLS employs a complicated handshake involving two exchanges of client-server messages.
        * multiple round-trip delays introduced by TLS handshake protocol.
        * A TLS-secured HTTP transaction may appear noticably slower for this reason.
        
DNS
    - Generally, the hosting platform provides a cache implementation to avoid frequent DNS queries. 
    - The semantics of DNS caching:
        * Each DNS response contains a time-to-live (TTL) attribute declaring how long the result may cached. 
        * TTLs can range from seconds to days but are typically on the order of several minutes. 
        * Very low TTL values, usually under a minute, are used to affect load-distribution or minimize downtime from server replacement or ISP failover.
    - Refresh on Failure of DNS cache entries
        * Highly-available systems usually rely upon redundant infrastructures hosted within their IP address space. 
        * Low-TTL DNS entries have the benefit of reducing the time a network client may refer to the address of a failed host, but at the same time triggers a lot of extra DNS queries. 
        * The TTL is a compromise between minimizing downtime and maximizing client performance.
        * It makes no sense to generally degrade client performance when server failures are the exception to the rule. 

