Q: [Closest pair] 
Given a set of points, find the pair that is closest (with either metric). 

A: Line sweep algorithm:

   This can be solved in O(N2) time by considering all the pairs, but a line sweep can reduce this to O(N log N).
   Suppose that we have processed points 1 to N − 1 (ordered by X) and the shortest distance we have found so far is h. 
   We now process point N and try to find a point closer to it than h. We maintain a set of all already-processed points 
   whose X coordinates are within h of point N. 
   As each point is processed, it is added to the set, and when we move on to the next point or when h is decreased, points are removed from the set. 
   ** The set is ordered by y coordinate. 
   
   A balanced binary tree is suitable for this, and accounts for the log N factor.
   
   Become a range search problem:
      - To search for points closer than h to point N, we need only consider points in the active set
      - furthermore we need only consider points whose y coordinates are in the range yN − h to yN + h

========================================================================
Q: Assume that we have N workers and N jobs that should be done. For each pair (worker, job) 
   we know salary that should be paid to worker for him to perform the job. 
   Our goal is to complete all jobs minimizing total inputs, while assigning each worker to exactly one job and vice versa.

A: Assignment Problem and Hungarian Algorithm 
   http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=hungarianAlgorithm

====================================================================
Q: An array with all the numbers from 1 to N, where N is at most 32,000. 
The array may have duplicate entries and you do not know what N is. With only 4KB of memory available, 
how would you print all duplicate elements in the array?

A: use bit vector to map 32000 integers.   32000/8 = 4K byte

void findDup()
{
    int i=1;
    std::bitset<N> s;
    for(int i=1; i<=N; i++) {
       if (s.test(i)) {
          std::cout << s[1] << "\n";
       } else {
          s.set(i);
       }
    }
}

=====================================================================
Q: How would you design the data structures for a very large social network (Facebook, LinkedIn, etc)? 
   Describe how you would design an algorithm to show the connection, or path, between two people 
   (e.g., Me -> Bob -> Susan -> Jason -> You).

A: When we deal with a service the size of Facebook, we cannot possibly keep all of our data on one machine. 
That means that our simple Person data structure from above doesn’t quite work — our friends may not live on 
the same machine as us. 

Instead, we can replace our list of friends with a list of their IDs, and traverse as follows:
1.	 For each friend ID: int machine_index = lookupMachineForUserID(id);
2.	 Go to machine machine_index
3.	 Person friend = lookupFriend(machine_index);

Optimization: Reduce Machine Jumps
- Jumping from one machine to another is expensive.  Instead of randomly jumping from machine to machine with each friend, 
  batch these jumps—e.g., if 5 of my friends live on one machine, I should look them up all at once.

Optimization: Smart Division of People and Machines (partition, vertically partition, database sharding)
  People are much more likely to be friends with people who live in the same country as them. 
  Rather than randomly dividing people up across machines, partition them up by country, city, state, etc. 
  This will reduce the number of jumps.

======================================================================
Q: finding running median from streamed data
A: Heap-based solution:
 
For the first two elements add smaller one to the maxHeap on the left, and bigger one to the minHeap on the right. 
Then process stream data one by one,

Step 1: Add next item to one of the heaps

   if (next item is smaller than maxHeap root)
      add it to maxHeap,
   else 
      add it to minHeap

Step 2: Balance the heaps (after this step heaps will be either balanced or one of them will contain 1 more item)

   if number of elements in one of the heaps is greater than the other by more than 1
   remove the root element from the one containing more elements and add to the other one

At any given time you can calculate median like this:

   If the heaps contain equal elements;
     median = (root of maxHeap + root of minHeap)/2
   Else
     median = root of the heap with more elements

Finding running median from a stream of data is a tough problem, and finding an exact solution with 
memory constraints efficiently is probably impossible for the general case.  On the other hand, if 
the data has some characteristics we can exploit, we can develop efficient specialized solutions. 
For example, if we know that the data is an integral type, then we can use counting sort, 
which can give you a constant memory constant time algorithm. 

Heap based solution is a more general solution because it can be used for other data types (doubles) as well. 
And finally, if the exact median is not required and an approximation is enough, you can just try to estimate 
a probability density function for the data and estimate median using that.


============================================================================================
Q: Suppose you have a long list of numbers and you want to find the 5th and 95th percentiles.
A: To make the problem tangible, suppose we have a 1000 numbers and we want to know 
   what the 50th number would be if we were to sort the list in increasing order. 
   Imagine we want to do this using as little memory as possible. We could read in the first 50 numbers and put into max-heap.
   Then when we read in the 51st number, we compare it to the largest of the 50 numbers we've saved (heap root). 
   If the new number is larger, we throw it away because we know it cannot be the 50th smallest number of the full list. 
   If the new number is smaller, we throw away the largest number we were keeping and insert the new number into heap.
   
   Same for the 95th percentile (only 5% of N numbers are needed).
  
    Same as you want to know N_th smallest/largest number of an infinite stream.

==============================================================================================
Q: Is there a way to measure how sorted a list is? 
  I mean, it's not about knowing if a list is sorted or not (boolean), but something like a ratio of "sortness", 
  something like the coefficient of correlation in statistics.
A: You can simply count the number of inversions in the list.
   Inversion: An inversion in a sequence of elements of type T is a pair of sequence elements that appear out of order 
   according to some ordering < on the set of T's.
   Example sequence 9, 5, 7, 6. 
      - This sequence has the inversions (0,1), (0,2), (0,3), (2,3) and the inversion number 4.

========================================================================================
Q: Have a huge file that cannot fit into memory. Each line has a string. How to sort the file?

A: Extenal sort. (Disk-based sorting)
   Assume the file is N. The available memory is M.
  
   Idea: 
    1. Divide the file into K chunks, where M * K = N. 
       Bring each chunk into memory and sort the lines as usual using any O(n log n) algorithm. 
       Save the lines back to the file.
    2. Now bring the next chunk into memory and sort.
    3. (K-way merge) Once we’re done, merge them one by one.

========================================================================================
Q: Given a pile of n pairs of socks, containing 2n elements (assume each sock has exactly one matching pair), 
   what is the best way to pair them up efficiently with up to logarithmic extra space?
A: Sorting solutions have been proposed but sorting is a little too much: 
   We don't need order, we just need equality groups.
   
   Hashing would be enough (and faster).
   Algorithm:
      step 1: For each color of socks, form a pile. Iterate over all socks in your input basket and distribute them onto the color piles.
      step 2: Iterate over each pile and distribute it by some other metric (e.g. pattern) into a second set of piles.
      step 3: Recursively apply this scheme until you have distributed all socks onto very small piles that you can visually process immediately
   
   This kind of recursive hash partitioning is actually being done by SQL Server when it needs to hash join or hash aggregate over huge data sets. It distributes its build input stream into many partitions which are independent. This scheme scales to arbitrary amounts of data and multiple CPUs linearly.
   You don't need recursive partitioning if you can find a distribution key (hash key) that provides enough buckets that each bucket is small enough to be processed very quickly. Unfortunately, I don't think socks have such a property.
   If each sock had an integer called "PairID" one could easily distribute them into 10 buckets according to PairID % 10 (the last digit).
   * The best real-world partitioning I can think of is creating a rectangle of piles: one dimension is color, the other is pattern. 
      Why a rectangle? Because we need O(1) random-access to piles. 
      (A 3D cuboid would also work, but that is not very practical.)

